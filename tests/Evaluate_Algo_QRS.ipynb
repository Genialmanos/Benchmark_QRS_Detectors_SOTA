{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0914813d-3153-4acf-8ab5-7dbf0ca39904",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wfdb\n",
    "import csv\n",
    "import os\n",
    "from scipy.signal import butter, filtfilt\n",
    "#from PyEMD import EMD\n",
    "import emd\n",
    "import time\n",
    "import biosppy.signals.ecg as bsp_ecg\n",
    "import biosppy.signals.tools as bsp_tools\n",
    "from sklearn.metrics import f1_score\n",
    "from hrvanalysis import remove_outliers, remove_ectopic_beats, interpolate_nan_values\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wfdb\n",
    "import csv\n",
    "import os\n",
    "from scipy.signal import butter, filtfilt\n",
    "import scipy\n",
    "#from PyEMD import EMD\n",
    "import emd\n",
    "import time\n",
    "from scipy.interpolate import CubicSpline\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "81f10ef2-62a1-4cf7-ba2f-ba69ee0a546a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'liste_num = [fichier[:-4] for fichier in os.listdir(\"data\") if fichier.endswith(\".dat\")]\\nfor num_patient in liste_num:\\n    patient_record = wfdb.rdrecord(f\"data/{num_patient}\")\\n    annotation = wfdb.rdann(f\\'data/{num_patient}\\', \\'atr\\')\\n    qrs = annotation.sample.tolist()\\n                            \\n    df = pd.DataFrame(patient_record.p_signal, columns= patient_record.sig_name)\\n    df_full = df.assign(labels= qrs + [None]*(df.shape[0]-len(qrs)))\\n    \\n    df_full.to_csv(f\"data_csv/{patient_record.record_name}.csv\", index=False)'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"liste_num = [fichier[:-4] for fichier in os.listdir(\"data\") if fichier.endswith(\".dat\")]\n",
    "for num_patient in liste_num:\n",
    "    patient_record = wfdb.rdrecord(f\"data/{num_patient}\")\n",
    "    annotation = wfdb.rdann(f'data/{num_patient}', 'atr')\n",
    "    qrs = annotation.sample.tolist()\n",
    "                            \n",
    "    df = pd.DataFrame(patient_record.p_signal, columns= patient_record.sig_name)\n",
    "    df_full = df.assign(labels= qrs + [None]*(df.shape[0]-len(qrs)))\n",
    "    \n",
    "    df_full.to_csv(f\"data_csv/{patient_record.record_name}.csv\", index=False)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedf9ee0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d88daf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamilton(signal, sampling_rate, group): \n",
    "    return bsp_ecg.ecg(signal=np.array(signal), sampling_rate=sampling_rate, show=False)[2]\n",
    "    rr_intervals_list = [(rr[i] - rr[i-1])/360 * 1000 for i in range(1,len(rr))]\n",
    "    rr_intervals_without_outliers = remove_outliers(\n",
    "        rr_intervals=rr_intervals_list,  \n",
    "        low_rri=300, high_rri=2000,\n",
    "        verbose= False)\n",
    "    interpolated_rr_intervals = interpolate_nan_values(rr_intervals=rr_intervals_without_outliers,\n",
    "                                                       interpolation_method=\"linear\")\n",
    "\n",
    "    nn_intervals_list = remove_ectopic_beats(rr_intervals=interpolated_rr_intervals, method=\"malik\",verbose= False)\n",
    "    interpolated_nn_intervals = interpolate_nan_values(rr_intervals=nn_intervals_list)\n",
    "    return [(sum(rr[:i])/1000)*360 for i in range(1,len(rr)+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b5184cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(signal, M, sampling_rate)\n",
    "data_tab = [\"mit_bih_supraventricular\", \"european\", \"mit_bih_long_term\", 'mit_bih_Arrhythmia'] # , \"mit_bih_noise_stress\",\n",
    "algo_tab = ['emd', 'hamilton']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0a85c9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emd = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ba326430",
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = \"emd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f2cc4294-266c-49bb-a9b2-77940cd419e6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   data    TP   FP    FN\n",
      "0   800  1804   79    89\n",
      "1   801   503   73  1988\n",
      "2   802  1650   25    18\n",
      "3   803   649   53  1410\n",
      "4   804   310    8  2514\n",
      "..  ...   ...  ...   ...\n",
      "73  890  1177  418   897\n",
      "74  891  2222  187   221\n",
      "75  892  1436   50  1387\n",
      "76  893   464   24  2006\n",
      "77  894  1918    7   440\n",
      "\n",
      "[78 rows x 4 columns]\n",
      "mit_bih_supraventricular\n",
      "f1_score = 72.96% / Sensi = 66.51% / Spec = 93.2%  //// 141.0 seconds\n",
      "     data     TP    FP    FN\n",
      "0   e0103   3628    46  3703\n",
      "1   e0104   6795   141   913\n",
      "2   e0105   6455  1647   248\n",
      "3   e0106   6654   864   510\n",
      "4   e0108   6433   259   171\n",
      "5   e0110   6829   683   264\n",
      "6   e0111   2079   251  5461\n",
      "7   e0112   2467   961  3126\n",
      "8   e0113   8857    36   313\n",
      "9   e0114    204     9  5394\n",
      "10  e0115  10667    64   695\n",
      "11  e0116   2873  1263  1656\n",
      "12  e0118    113   198  7106\n",
      "13  e0119    152   320  7921\n",
      "14  e0121   6944   194  3807\n",
      "15  e0122   5362    33  6046\n",
      "16  e0123   4545  1914  4659\n",
      "17  e0124   4021  1932  5273\n",
      "18  e0125   4296    41  4838\n",
      "19  e0126   3262  1734  5095\n",
      "20  e0127   5089     5  4398\n",
      "21  e0129   4962   644   765\n",
      "22  e0133   3897   466  2783\n",
      "23  e0136   4628  1166  2509\n",
      "24  e0139   2818    96  7964\n",
      "25  e0147   2574     8  3870\n",
      "26  e0148   5527   137  1262\n",
      "27  e0151    772   309  6914\n",
      "28  e0154   4813    97  2021\n",
      "29  e0155     10    20  8434\n",
      "30  e0159    306   187  9178\n",
      "31  e0161   8675  1021   264\n",
      "32  e0162   9566  1229  1161\n",
      "33  e0163   7514    24   211\n",
      "34  e0166    637  5525  5815\n",
      "35  e0170   7798   237  1357\n",
      "36  e0601   7002   611  1960\n",
      "37  e0602   7516   349  3726\n",
      "38  e0604   2406    17  5591\n",
      "39  e0605   3403    59  8276\n",
      "40  e0606   2293   737  7465\n",
      "41  e0609   7764   130  1677\n",
      "42  e0610   7159    44   908\n",
      "43  e0611      9    23  5947\n",
      "44  e0612   6261   303   841\n",
      "45  e0613   5416   214  2659\n",
      "46  e0615   4563  3168  2738\n",
      "european\n",
      "f1_score = 62.6% / Sensi = 56.03% / Spec = 82.63%  //// 795.0 seconds\n",
      "    data     TP    FP     FN\n",
      "0  14046  27628   671  87484\n",
      "1  14134  38674  5151  10677\n",
      "2  14149  60925  2463  78309\n",
      "3  14157  33283  4286  54277\n",
      "4  14172  16292  2036  49521\n",
      "5  14184  51116  5299  48215\n",
      "6  15814  79123  7966  22979\n",
      "mit_bih_long_term\n",
      "f1_score = 60.41% / Sensi = 48.26% / Spec = 91.56%  //// 1012.0 seconds\n",
      "   data    TP    FP    FN\n",
      "0   100  2244    75    11\n",
      "1   101  1832   155    22\n",
      "2   103  2087  2137    60\n",
      "3   105  1593   375  1011\n",
      "4   106  1487   560   579\n",
      "5   107   830  1397  1362\n",
      "6   108  1269   173   510\n",
      "7   109  2402    33   126\n",
      "8   111  1847  1896   109\n",
      "9   112  2286   457    69\n",
      "10  113  1862   711     2\n",
      "11  114  1862   239    20\n",
      "12  115  1731   326    13\n",
      "13  116  1638   369   699\n",
      "14  117  1607  2283     4\n",
      "15  118  1993   293   150\n",
      "16  119  1825   226   254\n",
      "17  121  1782   293    19\n",
      "18  122  2238   711    83\n",
      "19  123  1384   190    73\n",
      "20  124  1556   249    43\n",
      "21  200  1776   501   949\n",
      "22  201  1845   735   168\n",
      "23  202  1989   823    66\n",
      "24  203  1808   277  1242\n",
      "25  205  2632    44    41\n",
      "26  207  1756   247   558\n",
      "27  208  2317  1014   709\n",
      "28  209  2979   421    60\n",
      "29  210  2519   780   260\n",
      "30  212  2559   624   136\n",
      "31  213  2830  1866   306\n",
      "32  214  1929   598   169\n",
      "33  215  1882   778  1346\n",
      "34  217  1846   167   414\n",
      "35  219  2035   395   213\n",
      "36  220  2004   211    54\n",
      "37  221  2346   606   155\n",
      "38  222  2031   540   550\n",
      "39  223  2181   292   368\n",
      "40  228   364    77  1758\n",
      "41  230  2160   237   246\n",
      "42  231  1563    16   437\n",
      "43  232  1681   234    80\n",
      "44  233  2560   521   374\n",
      "45  234  2719   217    48\n",
      "mit_bih_Arrhythmia\n",
      "f1_score = 81.39% / Sensi = 85.7% / Spec = 80.23%  //// 256.0 seconds\n"
     ]
    }
   ],
   "source": [
    "#data_tab = ['european']\n",
    "dico_algo = {\"low_comput\": low_comput, \n",
    "             \"emd\" : full_emd, \n",
    "             \"hamilton\": hamilton, \n",
    "             \"wavelet\" : full_wavelets }\n",
    "sampling_rate = 360\n",
    "for data in data_tab:\n",
    "    start = time.time()\n",
    "    Result_DF = pd.DataFrame()\n",
    "\n",
    "    for fichier in os.listdir(f\"data_csv/{data}\"):\n",
    "        fs = 360\n",
    "        df = pd.read_csv(f'data_csv/{data}/{fichier}')\n",
    "\n",
    "        cible = \"\"\n",
    "        if data in ['mit_bih_supraventricular', \"mit_bih_long_term\"]:\n",
    "            cible = 'ECG1'\n",
    "            fs = 128\n",
    "            sampling_rate = 128\n",
    "        elif data == \"european\":\n",
    "            cible = \"MLIII\"\n",
    "            fs = 250\n",
    "            sampling_rate = 250\n",
    "        else:\n",
    "            cible = \"MLII\"\n",
    "\n",
    "        if cible not in df.columns:\n",
    "            continue\n",
    "\n",
    "        signal = np.array(df[cible], dtype=np.float32) #\"MLII\" # pour ventricular et long term : 'ECG1'\n",
    "        QRS = df[\"labels\"].dropna().tolist()\n",
    "        r_peaks = dico_algo[algo](signal, sampling_rate) #(signal, sampling_rate, 0)  # M, \n",
    "        #full_emd(signal, M, sampling_rate) ###\n",
    "        perf_temp = perf(QRS, r_peaks, 36)\n",
    "        Result_DF = pd.concat([Result_DF, pd.DataFrame({\"data\" : fichier[:-4], \"TP\": perf_temp[0], \"FP\": perf_temp[1], \"FN\":perf_temp[2]}, index=[0])], ignore_index=True)\n",
    "    print(Result_DF)\n",
    "    Result_DF[\"Sensi\"] = Result_DF[\"TP\"] / (Result_DF[\"TP\"] + Result_DF[\"FN\"])\n",
    "    Result_DF[\"Specificity\"] = Result_DF[\"TP\"] / (Result_DF[\"TP\"] + Result_DF[\"FP\"])\n",
    "    Result_DF[\"f1_score\"] = (2 * Result_DF[\"TP\"]) / (2 * Result_DF[\"TP\"] + Result_DF[\"FN\"] + Result_DF[\"FP\"])\n",
    "    Result_DF.sort_values(\"f1_score\", inplace = True)\n",
    "    end = time.time()\n",
    "    print(f\"{data}\")\n",
    "    print(f\"f1_score = {(np.mean(Result_DF['f1_score'].tolist()) * 100).round(2)}% / Sensi = {(np.mean(Result_DF['Sensi'].tolist())* 100).round(2)}% / Spec = {(np.mean(Result_DF['Specificity'].tolist())* 100).round(2)}%  //// {round((end-start), 0)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6395402-8323-4ba7-ba80-b8ff056b63e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wavelet 36:\n",
    "mit_bih_Arrhythmia\n",
    "f1_score = 97.34% / Sensi = 96.78% / Spec = 98.09%  //// 34.0 seconds\n",
    "mit_bih_long_term\n",
    "f1_score = 92.96% / Sensi = 87.39% / Spec = 99.75%  //// 266.0 seconds\n",
    "mit_bih_noise_stress\n",
    "f1_score = 88.49% / Sensi = 92.78% / Spec = 85.07%  //// 15.0 seconds\n",
    "ventricular : \n",
    "f1_score = 95.77% / Sensi = 92.76% / Spec = 99.49%  //// 16.0 seconds\n",
    "european\n",
    "f1_score = 94.4% / Sensi = 95.57% / Spec = 94.21%  //// 129.0 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e9d26a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# hamilton 36\n",
    "mit_bih_Arrhythmia\n",
    "f1_score = 97.73% / Sensi = 95.89% / Spec = 99.84%  //// 72.0 seconds\n",
    "dataset = mit_bih_long_term\n",
    "f1_score = 81.97% / Sensi = 71.93% / Spec = 99.93%  //// 1195.0 seconds\n",
    "dataset = mit_bih_noise_stress\n",
    "f1_score = 88.59% / Sensi = 89.25% / Spec = 88.1%  //// 20.0 seconds\n",
    "Ventricular :\n",
    "f1_score = 89.9% / Sensi = 84.18% / Spec = 99.37%  //// 53.0 seconds\n",
    "european\n",
    "f1_score = 95.88% / Sensi = 94.02% / Spec = 98.1%  //// 329.0 seconds\n",
    "\n",
    "# hamilton : 18\n",
    "mit_bih_Arrhythmia\n",
    "f1_score = 96.46% / Sensi = 94.69% / Spec = 98.5%  //// 68.0 seconds\n",
    "mit_bih_long_term\n",
    "f1_score = 73.66% / Sensi = 64.03% / Spec = 91.17%  //// 1391.0 seconds\n",
    "mit_bih_noise_stress\n",
    "f1_score = 86.84% / Sensi = 87.46% / Spec = 86.39%  //// 20.0 seconds\n",
    "mit_bih_supraventricular\n",
    "f1_score = 88.44% / Sensi = 82.72% / Spec = 97.93%  //// 53.0 seconds\n",
    "european\n",
    "f1_score = 86.16% / Sensi = 84.58% / Spec = 87.98%  //// 333.0 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc83be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# emd:\n",
    "mit_bih_Arrhythmia\n",
    "f1_score = 0.78% / Sensi = 5.15% / Spec = 0.68%  //// 860.0 seconds\n",
    "noise:\n",
    "f1_score = 0.76148 / Sensi = 0.75351 / Spec = 0.79285   /// 183.1329312324524\n",
    "ventricular : \n",
    "f1_score = 0.79018 / Sensi = 0.71223 / Spec = 0.98957   /// 336.951664686203\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc03b758",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perf(labels, peaks, minmax):\n",
    "    x = np.concatenate([np.array(labels), np.array(peaks)]) #list(set(QRS + r_peaks))\n",
    "    x.sort()\n",
    "    diff = x[1:]-x[:-1]\n",
    "    gps = np.concatenate([[0], np.cumsum(diff>=minmax)])\n",
    "    temp = [x[gps==i] for i in range(gps[-1]+1)]\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    list_F = []\n",
    "    for sublist in temp:\n",
    "        if len(sublist) == 2:\n",
    "            TP += 1\n",
    "        else:\n",
    "            list_F.append(sublist)\n",
    "            if sublist[0] in labels:\n",
    "                FN += 1\n",
    "            else:\n",
    "                FP += 1\n",
    "    return TP, FP, FN, list_F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd951b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def low_comput(ecg_signal, fs):\n",
    "    k = fs//50\n",
    "    filtered_signal = apply_fir_filter(ecg_signal, k)\n",
    "    squared_signal = filtered_signal ** 2\n",
    "    QRS_width = int(0.12 * fs)                                                    # 0.12 * fs\n",
    "    smoothed_signal = moving_average(squared_signal, QRS_width)\n",
    "    normalized_signal = normalize_operation(smoothed_signal)\n",
    "    thv = threshold(normalized_signal, QRS_width)\n",
    "    r_peaks_01 = [1 if thv[i] <= normalized_signal[i] else 0 for i in range(len(thv))]\n",
    "    r_peaks = [i for i in range(len(r_peaks_01)) if r_peaks_01[i] == 1]\n",
    "    r_peaks = regroup_nul(np.array(r_peaks), 36)\n",
    "    Rsi = int(0.24 * fs)\n",
    "    a = - Rsi\n",
    "    tab = []\n",
    "\n",
    "    for value in r_peaks:\n",
    "        if value >= a + Rsi:\n",
    "            tab.append(value)\n",
    "            a = value\n",
    "            \n",
    "    r_peaks_real = super_filtre(ecg_signal, tab, Rsi)\n",
    "    return r_peaks_real\n",
    "\n",
    "\n",
    "def super_filtre(ecg_signal, r_peaks, Rsi):\n",
    "    WQRS = Rsi-1 if Rsi%2 == 0 else Rsi         #(12)\n",
    "    H_WQRS = WQRS//2\n",
    "    list_peak = []\n",
    "    L = 10\n",
    "    for idx, peak in enumerate(r_peaks[10:]):\n",
    "        sous_tab = r_peaks[idx-L:idx+L]\n",
    "        \n",
    "        direction = ( (1/(2*L)) * sum([ecg_signal[aaa] for aaa in sous_tab]) ) #13.a\n",
    "        if list_peak == []:\n",
    "            for p in r_peaks[:10]:\n",
    "                list_peak.append(choose_max(ecg_signal, H_WQRS, p, -1))\n",
    "                continue\n",
    "        list_peak.append(choose_max(ecg_signal, H_WQRS, peak, direction))\n",
    "    \n",
    "    return list_peak\n",
    "\n",
    "def threshold(signal, QRS_width):\n",
    "    N = QRS_width\n",
    "    thv = [1]\n",
    "    for i in signal:\n",
    "        thv.append(((N-1) * thv[-1] + i )/ N)\n",
    "    return thv[1:]\n",
    "\n",
    "def regroup_nul(peaks, thr):\n",
    "    diff = peaks[1:]-peaks[:-1]\n",
    "    gps = np.concatenate([[0], np.cumsum(diff>=thr)])\n",
    "    temp = [peaks[gps==i] for i in range(gps[-1]+1)]\n",
    "    return [np.mean(sublist).astype(int) for sublist in temp]\n",
    "\n",
    "def choose_max(ecg_signal, H_WQRS, peak, direction):\n",
    "    if direction > 0:\n",
    "        return peak - H_WQRS + np.argmax(ecg_signal[peak-(H_WQRS):peak+(H_WQRS)])\n",
    "    else:\n",
    "        return peak - H_WQRS + np.argmin(ecg_signal[peak-(H_WQRS):peak+(H_WQRS)])\n",
    "    \n",
    "def apply_fir_filter(signal, k):\n",
    "    y0 = np.zeros_like(signal)\n",
    "    y0[k:] = signal[k:] - signal[:-k]\n",
    "    return y0\n",
    "\n",
    "def moving_average(signal, window_size):\n",
    "    return np.convolve(signal, np.ones(window_size)/window_size, mode='same')\n",
    "\n",
    "def normalize_operation(signal):\n",
    "    return (signal - np.min(signal)) / (np.max(signal) - np.min(signal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dde587dc-5287-4e94-a338-fa949d364aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_emd(signal, sampling_rate):\n",
    "    signal_without_baseline =  baseline_wander_cancel(signal, sampling_rate)\n",
    "    l = []\n",
    "    imfs = emd.sift.sift(signal_without_baseline)\n",
    "    for i in range(0, 3):\n",
    "        imf = [t[i] for t in imfs]    \n",
    "        transformed_signal = nonlinear_transform(imf)\n",
    "        transformed_signal = normalize_data(transformed_signal)\n",
    "        M = 10\n",
    "        integrated_signal = integration(transformed_signal, M)\n",
    "        l.append(integrated_signal)\n",
    "    arr = np.array(l)\n",
    "    signal_end = arr.sum(axis=0)\n",
    "    filtered_signal, r_peaks = low_pass_filter_and_r_position(signal_end , sampling_rate, 0.3, 400)\n",
    "    return r_peaks\n",
    "\n",
    "def recheck(r_peaks, signal, fs):\n",
    "    rr_interval = np.diff(r_peaks)\n",
    "    #search_interval = int(v * np.mean(rr_interval))\n",
    "    missed_peaks = []\n",
    "    w = int(fs*0.2)\n",
    "    \n",
    "    for i in range(1, len(r_peaks) - 1):\n",
    "        if r_peaks[i+1] - r_peaks[i] > rr_interval[i-1] * 2:\n",
    "            intervening_segment = signal[r_peaks[i]+w:r_peaks[i+1]-w]\n",
    "            threshold = np.max(intervening_segment) * 0.3\n",
    "            #a = qrs_localization_normal(ecg_signal, (max(ecg_signal)*0.3*0.5)\n",
    "            a = qrs_localization(intervening_segment, threshold, fs)\n",
    "            missed_peaks.extend([r_peaks[i] + x for x in a])\n",
    "        missed_peaks.append(r_peaks[-1])\n",
    "    return missed_peaks\n",
    "\n",
    "\n",
    "def baseline_wander_cancel(ecg_signal, sampling_rate):\n",
    "    cutoff_freq = 1  # Fréquence de coupure à 1 Hz\n",
    "    order = 5  # Ordre du filtre\n",
    "    b, a = scipy.signal.butter(order, \n",
    "                         cutoff_freq / (sampling_rate / 2), \n",
    "                         'highpass', \n",
    "                         analog=False)\n",
    "    ecghp = scipy.signal.lfilter(b, a, ecg_signal)\n",
    "    return ecghp\n",
    "\n",
    "def low_pass_filter_and_r_position(signal, sampling_rate, seuil, w):\n",
    "    cutoff = 50\n",
    "    nyquist = 0.5 * sampling_rate\n",
    "    normal_cutoff = cutoff / nyquist\n",
    "    b, a = butter(1, normal_cutoff, btype='low', analog=False)\n",
    "    filtered_signal = filtfilt(b, a, signal)\n",
    "\n",
    "    derivative = np.diff(filtered_signal)\n",
    "    squared = derivative ** 2\n",
    "    #squared = np.clip(squared, a_min=None, a_max=0.0001)\n",
    "    t = np.mean(squared)\n",
    "    #print_signal(squared[:20000], \"squared\", print_qrs = False)\n",
    "    r_peaks = qrs_localization(squared, np.mean(squared), sampling_rate) # np.mean(squared)) \n",
    "    return squared, r_peaks\n",
    "\n",
    "def nonlinear_transform(x):\n",
    "    y = [0] * len(x)\n",
    "    for n in range(2, len(x)):\n",
    "        if (x[n] >= 0 and x[n-1] >= 0 and x[n-2] >= 0) or (x[n] <= 0 and x[n-1] <= 0 and x[n-2] <= 0):\n",
    "            y[n] = abs(x[n] * x[n-1] * x[n-2])\n",
    "    return y\n",
    "\n",
    "def integration(data, window_size):\n",
    "    result = np.cumsum(data)\n",
    "    result[window_size:] = result[window_size:] - result[:-window_size]\n",
    "    return result[window_size - 1:] / window_size\n",
    "\n",
    "def qrs_localization(h, XX, fs):\n",
    "    qrs_indices = []\n",
    "    sous_groupe = []\n",
    "    #w = 1000\n",
    "    threshold = XX # * max(h[:w])\n",
    "    A = int(fs//10)\n",
    "    compteur = 0\n",
    "    for i in range(len(h)):\n",
    "        if compteur != 0:\n",
    "            compteur -= 1\n",
    "            continue\n",
    "        #if i%w == 0 and i != 0:\n",
    "        #    threshold = XX * max(h[i-(w//2):i+(w//2)])\n",
    "        if h[i] >= threshold:\n",
    "            sous_groupe.append(i)\n",
    "        elif sous_groupe != [] and i - sous_groupe[-1] >= A: # 36 = valeur donnée dans l'article\n",
    "            v = max([(x, h[x]) for x in sous_groupe], key=lambda x: x[1])[0]\n",
    "            qrs_indices.append(v)\n",
    "            compteur = A\n",
    "            sous_groupe = []\n",
    "    if sous_groupe != []:\n",
    "        qrs_indices.append(max([(x, h[x]) for x in sous_groupe], key=lambda x: x[1])[0])\n",
    "    return qrs_indices\n",
    "\n",
    "def normalize_data(signal):\n",
    "    max_value = 0.7 * np.max(signal)\n",
    "    normalized_signal = np.where(signal >= max_value, 1, signal / max_value)\n",
    "    return normalized_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fff919-0e7e-4e4c-8fce-6edafc2a0246",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "288562f7-8cda-4d41-a9bd-0ddcfbb4a91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pywt\n",
    "def wavelet_decomposition(ecg_signal, idx):\n",
    "    coeffs = pywt.wavedec(ecg_signal, 'haar', level=idx)\n",
    "    d4 = extend_list_with_averages(coeffs[-2])\n",
    "    d4.insert(0,0)\n",
    "    h = np.abs(np.multiply(d4, coeffs[-1]))\n",
    "    return h\n",
    "\n",
    "def qrs_localization(h, thresold):\n",
    "    qrs_indices = []\n",
    "    sous_groupe = []\n",
    "    w = 1000\n",
    "    threshold = 0.15 * max(h[:w])\n",
    "    for i in range(len(h)):\n",
    "        if i%w == 0 and i != 0:\n",
    "            threshold = 0.15 * max(h[i-(w//2):i+(w//2)])\n",
    "        #print(h[i])\n",
    "        if h[i] >= threshold:\n",
    "            sous_groupe.append(i)\n",
    "        elif sous_groupe != [] and i - sous_groupe[-1] >= 36: # 36 = valeur donnée dans l'article\n",
    "            qrs_indices.append(max([(x, h[x]) for x in sous_groupe], key=lambda x: x[1])[0])\n",
    "            sous_groupe = []\n",
    "    if sous_groupe != []:\n",
    "        qrs_indices.append(max([(x, h[x]) for x in sous_groupe], key=lambda x: x[1])[0])\n",
    "    return qrs_indices\n",
    "\n",
    "def delete_contraction(r_peaks, fs):\n",
    "    new_r_peaks = []\n",
    "    i = 0\n",
    "    while i < len(r_peaks) -1:\n",
    "        new_r_peaks.append(r_peaks[i])\n",
    "        if r_peaks[i] >= r_peaks[i+1] + int(fs/5): # 5 * (fs / 2 )200 correspond à un mouvement du coeur particulier après un battement\n",
    "            i += 2\n",
    "        else:\n",
    "            i += 1\n",
    "    return new_r_peaks\n",
    "            \n",
    "def searchback_missed_qrs(ecg_signal, filtered_indices, fs):\n",
    "    rr_interval = np.mean(np.diff(filtered_indices))\n",
    "    search_interval = int(1.5 * rr_interval)\n",
    "    secondary_threshold = 0.5 * max(ecg_signal)\n",
    "    missed_peaks = []\n",
    "    for i in range(len(filtered_indices) - 1):\n",
    "        if filtered_indices[i+1] - filtered_indices[i] > search_interval:\n",
    "            intervening_segment = ecg_signal[filtered_indices[i]:filtered_indices[i+1]]\n",
    "            missed_peaks.extend(qrs_localization(intervening_segment, secondary_threshold))\n",
    "    return missed_peaks\n",
    "\n",
    "def extend_list_with_averages(input_list):\n",
    "    extended_list = [input_list[0]]\n",
    "    length = len(input_list)\n",
    "\n",
    "    for i in range(length - 1):\n",
    "        current_element = input_list[i]\n",
    "        next_element = input_list[i + 1]\n",
    "        average = (current_element + next_element) / 2\n",
    "        extended_list.append(average)\n",
    "        extended_list.append(next_element)\n",
    "\n",
    "    return extended_list\n",
    "\n",
    "def full_wavelets(signal, fs, group):\n",
    "    h = wavelet_decomposition(signal, 5)\n",
    "    h = extend_list_with_averages(h)\n",
    "    threshold = 0.3 * max(h)\n",
    "    qrs = np.array(qrs_localization(h, threshold))\n",
    "    qrs.sort()\n",
    "    qrs = delete_contraction(qrs, sampling_rate)\n",
    "    qrs = qrs + searchback_missed_qrs(h, qrs, fs)\n",
    "    return qrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c31dde8-6800-4f78-8afe-788a842eb26d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fe7c4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
